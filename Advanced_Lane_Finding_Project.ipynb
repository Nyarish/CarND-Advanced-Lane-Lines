{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Lane Finding Project\n",
    "\n",
    "In this project, the objective is to detect lane lines in images using Python and OpenCV. OpenCV means \"Open-Source Computer Vision\", which is a package that has many useful tools for analyzing images.\n",
    "\n",
    "The goal and steps of this project are the following:\n",
    "\n",
    "* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
    "* Apply a distortion correction to raw images.\n",
    "* Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "* Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "* Detect lane pixels and fit to find the lane boundary.\n",
    "* Determine the curvature of the lane and vehicle position with respect to center.\n",
    "* Warp the detected lane boundaries back onto the original image.\n",
    "* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get imports\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Compute the camera calibration using chessboard images\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function \"objectAndImagePoints\" to Calibrate camera using ChaseBoard\n",
    "\n",
    "def objectAndImagePoints(image_dir):\n",
    "    \n",
    "    \"\"\" \n",
    "    This function takes in a set of images used for calibration,\n",
    "    and outputs objpoints, imgpoints and corners to compute the \n",
    "    camera calibration and distortion coefficients using the cv2.calibrateCamera() function.\n",
    "\n",
    "    input: images\n",
    "    args: \n",
    "    output:objpoints, imgpoints, corners\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "    objp = np.zeros((6*9,3), np.float32)\n",
    "    objp[:,:2] = np.mgrid[0:9, 0:6].T.reshape(-1,2)\n",
    "    \n",
    "    # Arrays to store object points and image points from all the images.\n",
    "    objpoints = [] # 3d points in real world space\n",
    "    imgpoints = [] # 2d points in image plane.\n",
    "    \n",
    "    # Make a list of calibration images\n",
    "    images = image_dir\n",
    "    \n",
    "    # Step through the list and search for chessboard corners\n",
    "    for idx, fname in enumerate(images):\n",
    "        img = cv2.imread(fname)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Find the chessboard corners\n",
    "        ret, corners = cv2.findChessboardCorners(gray, (9,6), None)\n",
    "\n",
    "        # If found, add object points, image points\n",
    "        if ret == True:\n",
    "            objpoints.append(objp)\n",
    "            imgpoints.append(corners)\n",
    "\n",
    "            # Draw and display the corners\n",
    "            cv2.drawChessboardCorners(img, (9,6), corners, ret)\n",
    "            cv2.imshow('img', img)\n",
    "            cv2.waitKey(500)\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    \n",
    "    return objpoints, imgpoints, corners\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory for calibration images\n",
    "image_dir = glob.glob('camera_cal/calibration*.jpg')\n",
    "# Get the objpoints, imgpoints, corners for use in distortion correction. \n",
    "# objpoints, imgpoints, corners = objectAndImagePoints(image_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Distortion correction\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function \"calibrate_undistort\" to Calibrate camera using ChaseBoard\n",
    "def calibrate_undistort(img):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function takes image, objpoints, imgpoints as input, it then outputs undistorted image.\n",
    "    It then uses the cv2.calibrateCamera() function on the inputs for distortion correction on\n",
    "    the image using the cv2.undistort() function and obtains an outpur result.\n",
    "    \n",
    "    Input:img, objpoints, imgpoints\n",
    "    args:\n",
    "    Output: undistorted image\n",
    "    \n",
    "    \"\"\"\n",
    "    img = np.copy(img)\n",
    "    img_size = (img.shape[1], img.shape[0])\n",
    "    \n",
    "    # Get the objpoints, imgpoints for use in distortion correction. \n",
    "    objpoints, imgpoints,_ = objectAndImagePoints(image_dir)\n",
    "    \n",
    "    # Do camera calibration given object points and image points\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, img_size,None,None)\n",
    "    \n",
    "    dst = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    dst = cv2.cvtColor(dst, cv2.COLOR_BGR2RGB)\n",
    "    return dst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeUndistortion(original_image, dist_image):\n",
    "    # Visualize undistortion\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
    "    ax1.imshow(original_image);\n",
    "    ax1.set_title('Original Image', fontsize=30)\n",
    "\n",
    "    ax2.imshow(dist_image);\n",
    "    ax2.set_title('Undistorted Image', fontsize=30)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pipelineUndistort(img, destination_path, save_name):\n",
    "    dst = calibrate_undistort(img)\n",
    "    cv2.imwrite(destination_path + save_name, dst)\n",
    "    \n",
    "    undist_img = mpimg.imread(destination_path + save_name)\n",
    "    \n",
    "    return visualizeUndistortion(img, undist_img)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Files to undistort\n",
    "img_cal = mpimg.imread('./camera_cal/calibration1.jpg')\n",
    "img_straight_lines1 = mpimg.imread('./test_images/straight_lines1.jpg')\n",
    "img_straight_lines2 = mpimg.imread('./test_images/straight_lines2.jpg')\n",
    "img_test1 = mpimg.imread('./test_images/test1.jpg')\n",
    "img_test2 = mpimg.imread('./test_images/test2.jpg')\n",
    "img_test3 = mpimg.imread('./test_images/test3.jpg')\n",
    "img_test4 = mpimg.imread('./test_images/test4.jpg')\n",
    "img_test5 = mpimg.imread('./test_images/test5.jpg')\n",
    "img_test6 = mpimg.imread('./test_images/test6.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to saved directoy \n",
    "dir_path = './output_images/'\n",
    "save_name = \"\"\n",
    "\n",
    "\n",
    "# Read undisorted files\n",
    "undist_img_cal = pipelineUndistort(img_cal, dir_path, save_name='undistorted_cal.jpg')\n",
    "undist_straight_lines1 = pipelineUndistort(img_straight_lines1, dir_path, save_name='undist_straight_lines1.jpg')\n",
    "undist_straight_lines2 = pipelineUndistort(img_straight_lines2, dir_path, save_name='undist_straight_lines2.jpg')\n",
    "\n",
    "undist_img_test1 = pipelineUndistort(img_test1,dir_path, save_name='undist_img_test1.jpg')\n",
    "undist_img_test2 = pipelineUndistort(img_test2, dir_path, save_name='undist_img_test2.jpg')\n",
    "undist_img_test3 = pipelineUndistort(img_test3,dir_path, save_name='undist_img_test3.jpg')\n",
    "undist_img_test4 = pipelineUndistort(img_test4,dir_path, save_name='undist_img_test4.jpg')\n",
    "undist_img_test5 = pipelineUndistort(img_test5,dir_path, save_name='undist_img_test5.jpg')\n",
    "undist_img_test6 = pipelineUndistort(img_test6,dir_path, save_name='undist_img_test6.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Color and gradient threshold\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read image to use in color and gradient threshold\n",
    "undistorted_straight_lines1 = mpimg.imread('./output_images/undist_straight_lines1.jpg')\n",
    "plt.imshow(undistorted_straight_lines1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def pipelineColorAndGradientThreshold(image, s_thresh=(170, 255), sx_thresh=(20, 100)):\n",
    "    img = np.copy(image)\n",
    "    # Convert to HLS color space and separate the V channel\n",
    "    hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "    l_channel = hls[:,:,1]\n",
    "    s_channel = hls[:,:,2]\n",
    "    \n",
    "    # Sobel x\n",
    "    sobelx = cv2.Sobel(l_channel, cv2.CV_64F, 1, 0) # Take the derivative in x\n",
    "    abs_sobelx = np.absolute(sobelx) # Absolute x derivative to accentuate lines away from horizontal\n",
    "    scaled_sobel = np.uint8(255*abs_sobelx/np.max(abs_sobelx))\n",
    "    \n",
    "    # Threshold x gradient\n",
    "    sxbinary = np.zeros_like(scaled_sobel)\n",
    "    sxbinary[(scaled_sobel >= sx_thresh[0]) & (scaled_sobel <= sx_thresh[1])] = 1\n",
    "    \n",
    "    # Threshold color channel\n",
    "    s_binary = np.zeros_like(s_channel)\n",
    "    s_binary[(s_channel >= s_thresh[0]) & (s_channel <= s_thresh[1])] = 1\n",
    "    # Stack each channel\n",
    "    color_binary = np.dstack(( np.zeros_like(sxbinary), sxbinary, s_binary)) * 255\n",
    "    \n",
    "    \n",
    "    # Combine the two binary thresholds\n",
    "    binary_image = np.zeros_like(sxbinary)\n",
    "    binary_image[(s_binary == 1) | (sxbinary == 1)] = 1\n",
    "    return color_binary, binary_image\n",
    "    \n",
    "def visualizeColorTheshold(image1, image2):\n",
    "    # Plot the result\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    f.tight_layout()\n",
    "\n",
    "    ax1.imshow(image1)\n",
    "    ax1.set_title('Color_binary', fontsize=15)\n",
    "\n",
    "    ax2.imshow(image2, cmap='gray')\n",
    "    ax2.set_title('Binary_image', fontsize=15)\n",
    "    # plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_binary, binary_image = pipelineColorAndGradientThreshold(undistorted_straight_lines1)\n",
    "visualizeColorTheshold(color_binary, binary_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Perspective transform\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "import perspective_transform_functions as ptf\n",
    "import draw_perspective_lines as dpl\n",
    "\n",
    "def visualizePerspectiveTransform(image1, image2):\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    f.tight_layout()\n",
    "    ax1.imshow(image1)\n",
    "    ax1.set_title('Original Undistorted Image', fontsize=15)\n",
    "    ax2.imshow(image2)\n",
    "    ax2.set_title('Undistorted and Warped Image', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get region of interest source and destination \n",
    "src, dst = ptf.get_trapezoid(undistorted_straight_lines1)\n",
    "src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw sraight lane lines on edge of the road\n",
    "masked_image = dpl.region_of_interest(binary_image, src)\n",
    "plt.imshow(masked_image);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Hough transform parameters\n",
    "\n",
    "rho = 2 # distance resolution in pixels of the Hough grid\n",
    "theta = np.pi/180 # angular resolution in radians of the Hough grid\n",
    "threshold = 10    # minimum number of votes (intersections in Hough grid cell)\n",
    "min_line_len = 10 #minimum number of pixels making up a line\n",
    "max_line_gap = 20    # maximum gap in pixels between connectable line segments\n",
    "\n",
    "\n",
    "# Run Hough on edge detected image\n",
    "# Output \"lines\" is an array containing endpoints of detected line segments\n",
    "line_img, lines = dpl.hough_lines(masked_image, rho, theta, threshold, min_line_len, max_line_gap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(line_img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continous_lines = dpl.getLeft_and_rightLane(undistorted_straight_lines1, lines)\n",
    "final_image = dpl.drawSingleft_and_rightLane(undistorted_straight_lines1,continous_lines,thickness=6,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(final_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perspective transform on drawn image\n",
    "# unwarped = ptf.get_original_perspective(img)\n",
    "top_down = ptf.get_transformed_perspective(final_image)\n",
    "visualizePerspectiveTransform(final_image, top_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perspective transform on lane lines. \n",
    "# unwarped = ptf.get_original_perspective(img)\n",
    "top_down = ptf.get_transformed_perspective(binary_image)#undistorted_straight_lines1\n",
    "birds_view = ptf.get_transformed_perspective(undistorted_straight_lines1)\n",
    "visualizePerspectiveTransform(undistorted_straight_lines1, birds_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Detect lane pixels and fit to find the lane boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram = np.sum(top_down[top_down.shape[0]//2:,:], axis=0)\n",
    "plt.plot(histogram);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "import detect_lane_pixels as detect_lane_pixels\n",
    "\n",
    "# Visualize lane window\n",
    "out_img,left_fit, right_fit = detect_lane_pixels.fit_polynomial(top_down)\n",
    "# out_img,left_fit, right_fit = fit_polynomial(top_down)\n",
    "plt.imshow(out_img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run image through the pipeline show lane pixels on road\n",
    "result,left_fitx, right_fitx, ploty = detect_lane_pixels.search_around_poly(top_down)\n",
    "\n",
    "# View your output\n",
    "plt.imshow(result);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Determine the curvature of the lane and vehicle position with respect to center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_curvature_pixels(image):#binary_warped\n",
    "    '''\n",
    "    Calculates the curvature of polynomial functions in pixels.\n",
    "    '''\n",
    "    \n",
    "    result,left_fitx, right_fitx, ploty = detect_lane_pixels.search_around_poly(image)\n",
    "    \n",
    "    # Define y-value where we want radius of curvature\n",
    "    # We'll choose the maximum y-value, corresponding to the bottom of the image\n",
    "    y_eval = np.max(ploty)\n",
    "    \n",
    "    # Calculation of R_curve (radius of curvature)\n",
    "    left_curverad = ((1 + (2*left_fit[0]*y_eval + left_fit[1])**2)**1.5) / np.absolute(2*left_fit[0])\n",
    "    right_curverad = ((1 + (2*right_fit[0]*y_eval + right_fit[1])**2)**1.5) / np.absolute(2*right_fit[0])\n",
    "    \n",
    "    return left_curverad, right_curverad\n",
    "\n",
    "\n",
    "# Calculate the radius of curvature in pixels for both lane lines\n",
    "left_curverad, right_curverad = measure_curvature_pixels(top_down)\n",
    "\n",
    "print(left_curverad, right_curverad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_curvature_real(image):#binary_warped\n",
    "    '''\n",
    "    Calculates the curvature of polynomial functions in meters.\n",
    "    '''\n",
    "    \n",
    "    image_center = image.shape[1] / 2\n",
    "    #lane_width_px = \n",
    "    # Define conversions in x and y from pixels space to meters\n",
    "    ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "    xm_per_pix = 3.7/700 # meters per pixel in x dimension\n",
    "    \n",
    "    result,left_fit_cr, right_fit_cr, ploty = detect_lane_pixels.search_around_poly(image)\n",
    "    \n",
    "    # Define y-value where we want radius of curvature\n",
    "    # We'll choose the maximum y-value, corresponding to the bottom of the image\n",
    "    y_eval = np.max(ploty)\n",
    "    \n",
    "    # Calculation of R_curve (radius of curvature)\n",
    "    left_curverad = ((1 + (2*left_fit_cr[0]*y_eval*ym_per_pix + left_fit_cr[1])**2)**1.5) / np.absolute(2*left_fit_cr[0])\n",
    "    right_curverad = ((1 + (2*right_fit_cr[0]*y_eval*ym_per_pix + right_fit_cr[1])**2)**1.5) / np.absolute(2*right_fit_cr[0])\n",
    "    \n",
    "    \n",
    "    mean_curvature = np.int32(np.mean((left_curverad, right_curverad)))\n",
    "    \n",
    "    # Mean position of the lanes in the image\n",
    "    leftx = left_fit_cr\n",
    "    rightx = right_fit_cr\n",
    "    lane_centerx = np.mean((leftx[-1], rightx[-1]))\n",
    "    \n",
    "    # Lane mean position relative to image center. \n",
    "    # If positive, the lane is offset to the left i.e. positive vehicle is to the right.\n",
    "    vehicle_position_px = image_center - lane_centerx\n",
    "    vehicle_position_cm = np.int32(vehicle_position_px * xm_per_pix * 100)\n",
    "    \n",
    "    \n",
    "    return left_curverad, right_curverad #left_curverad, right_curverad\n",
    "\n",
    "\n",
    "# # Calculate the radius of curvature in meters for both lane lines\n",
    "left_curverad, right_curverad = measure_curvature_real(top_down)\n",
    "print(left_curverad, 'm', right_curverad, 'm')\n",
    "\n",
    "\n",
    "# left_curverad, right_curverad, vehicle_position_cm = measure_curvature_real(top_down)\n",
    "# print(left_curverad, 'm', right_curverad, 'm', vehicle_position_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
