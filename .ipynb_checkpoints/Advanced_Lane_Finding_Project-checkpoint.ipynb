{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Lane Finding Project\n",
    "\n",
    "In this project, the objective is to detect lane lines in images using Python and OpenCV. OpenCV means \"Open-Source Computer Vision\", which is a package that has many useful tools for analyzing images.\n",
    "\n",
    "The goal and steps of this project are the following:\n",
    "\n",
    "* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
    "* Apply a distortion correction to raw images.\n",
    "* Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "* Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "* Detect lane pixels and fit to find the lane boundary.\n",
    "* Determine the curvature of the lane and vehicle position with respect to center.\n",
    "* Warp the detected lane boundaries back onto the original image.\n",
    "* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get imports\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Compute the camera calibration using chessboard images\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function \"objectAndImagePoints\" to Calibrate camera using ChaseBoard\n",
    "\n",
    "def objectAndImagePoints(image_dir):\n",
    "    \n",
    "    \"\"\" \n",
    "    This function takes in a set of images used for calibration,\n",
    "    and outputs objpoints, imgpoints and corners to compute the \n",
    "    camera calibration and distortion coefficients using the cv2.calibrateCamera() function.\n",
    "\n",
    "    input: images\n",
    "    args: \n",
    "    output:objpoints, imgpoints, corners\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "    objp = np.zeros((6*9,3), np.float32)\n",
    "    objp[:,:2] = np.mgrid[0:9, 0:6].T.reshape(-1,2)\n",
    "    \n",
    "    # Arrays to store object points and image points from all the images.\n",
    "    objpoints = [] # 3d points in real world space\n",
    "    imgpoints = [] # 2d points in image plane.\n",
    "    \n",
    "    # Make a list of calibration images\n",
    "    images = image_dir\n",
    "    \n",
    "    # Step through the list and search for chessboard corners\n",
    "    for idx, fname in enumerate(images):\n",
    "        img = cv2.imread(fname)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Find the chessboard corners\n",
    "        ret, corners = cv2.findChessboardCorners(gray, (9,6), None)\n",
    "\n",
    "        # If found, add object points, image points\n",
    "        if ret == True:\n",
    "            objpoints.append(objp)\n",
    "            imgpoints.append(corners)\n",
    "\n",
    "            # Draw and display the corners\n",
    "            cv2.drawChessboardCorners(img, (9,6), corners, ret)\n",
    "            cv2.imshow('img', img)\n",
    "            cv2.waitKey(500)\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    \n",
    "    return objpoints, imgpoints, corners\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory for calibration images\n",
    "image_dir = glob.glob('camera_cal/calibration*.jpg')\n",
    "# Get the objpoints, imgpoints, corners for use in distortion correction. \n",
    "objpoints, imgpoints, corners = objectAndImagePoints(image_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Distortion correction\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function \"calibrate_undistort\" to Calibrate camera using ChaseBoard\n",
    "def calibrate_undistort(img, objpoints, imgpoints):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function takes image, objpoints, imgpoints as input, it then outputs undistorted image.\n",
    "    It then uses the cv2.calibrateCamera() function on the inputs for distortion correction on\n",
    "    the image using the cv2.undistort() function and obtains an outpur result.\n",
    "    \n",
    "    Input:img, objpoints, imgpoints\n",
    "    args:\n",
    "    Output: undistorted image\n",
    "    \n",
    "    \"\"\"\n",
    "    img = np.copy(img)\n",
    "    img_size = (img.shape[1], img.shape[0])\n",
    "    \n",
    "    # Do camera calibration given object points and image points\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, img_size,None,None)\n",
    "    \n",
    "    dst = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    dst = cv2.cvtColor(dst, cv2.COLOR_BGR2RGB)\n",
    "    return dst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeUndistortion(original_image, dist_image):\n",
    "    # Visualize undistortion\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
    "    ax1.imshow(original_image);\n",
    "    ax1.set_title('Original Image', fontsize=30)\n",
    "\n",
    "    ax2.imshow(dist_image);\n",
    "    ax2.set_title('Undistorted Image', fontsize=30)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pipelineUndistort(img, objpoints, imgpoints, destination_path, save_name):\n",
    "    dst = calibrate_undistort(img, objpoints, imgpoints)\n",
    "    cv2.imwrite(destination_path + save_name, dst)\n",
    "    \n",
    "    undist_img = mpimg.imread(destination_path + save_name)\n",
    "    \n",
    "    return visualizeUndistortion(img, undist_img)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Files to undistort\n",
    "img_cal = mpimg.imread('./camera_cal/calibration1.jpg')\n",
    "img_straight_lines1 = mpimg.imread('./test_images/straight_lines1.jpg')\n",
    "img_straight_lines2 = mpimg.imread('./test_images/straight_lines2.jpg')\n",
    "img_test1 = mpimg.imread('./test_images/test1.jpg')\n",
    "img_test2 = mpimg.imread('./test_images/test2.jpg')\n",
    "img_test3 = mpimg.imread('./test_images/test3.jpg')\n",
    "img_test4 = mpimg.imread('./test_images/test4.jpg')\n",
    "img_test5 = mpimg.imread('./test_images/test5.jpg')\n",
    "img_test6 = mpimg.imread('./test_images/test6.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to saved directoy \n",
    "destination_path = './output_images/'\n",
    "save_name = \"\"\n",
    "\n",
    "\n",
    "# Read undisorted files\n",
    "undist_img_cal = pipelineUndistort(img_cal, objpoints, imgpoints, destination_path, \n",
    "                          save_name='undistorted_cal.jpg')\n",
    "undist_straight_lines1 = pipelineUndistort(img_straight_lines1, objpoints, imgpoints, destination_path, \n",
    "                                  save_name='undist_straight_lines1.jpg')\n",
    "undist_straight_lines2 = pipelineUndistort(img_straight_lines2, objpoints, imgpoints, destination_path, \n",
    "                                  save_name='undist_straight_lines2.jpg')\n",
    "undist_img_test1 = pipelineUndistort(img_test1, objpoints, imgpoints, destination_path, \n",
    "                                  save_name='undist_img_test1.jpg')\n",
    "undist_img_test2 = pipelineUndistort(img_test2, objpoints, imgpoints, destination_path, \n",
    "                                  save_name='undist_img_test2.jpg')\n",
    "undist_img_test3 = pipelineUndistort(img_test3, objpoints, imgpoints, destination_path, \n",
    "                                  save_name='undist_img_test3.jpg')\n",
    "undist_img_test4 = pipelineUndistort(img_test4, objpoints, imgpoints, destination_path, \n",
    "                                  save_name='undist_img_test4.jpg')\n",
    "undist_img_test5 = pipelineUndistort(img_test5, objpoints, imgpoints, destination_path, \n",
    "                                  save_name='undist_img_test5.jpg')\n",
    "undist_img_test6 = pipelineUndistort(img_test6, objpoints, imgpoints, destination_path, \n",
    "                                  save_name='undist_img_test6.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Color and gradient threshold\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "undistorted_straight_lines2 = mpimg.imread('./output_images/undist_straight_lines2.jpg')\n",
    "plt.imshow(undistorted_straight_lines2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def pipeline(image, s_thresh=(170, 255), sx_thresh=(20, 100)):\n",
    "    img = np.copy(image)\n",
    "    # Convert to HLS color space and separate the V channel\n",
    "    hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "    l_channel = hls[:,:,1]\n",
    "    s_channel = hls[:,:,2]\n",
    "    \n",
    "    # Sobel x\n",
    "    sobelx = cv2.Sobel(l_channel, cv2.CV_64F, 1, 0) # Take the derivative in x\n",
    "    abs_sobelx = np.absolute(sobelx) # Absolute x derivative to accentuate lines away from horizontal\n",
    "    scaled_sobel = np.uint8(255*abs_sobelx/np.max(abs_sobelx))\n",
    "    \n",
    "    # Threshold x gradient\n",
    "    sxbinary = np.zeros_like(scaled_sobel)\n",
    "    sxbinary[(scaled_sobel >= sx_thresh[0]) & (scaled_sobel <= sx_thresh[1])] = 1\n",
    "    \n",
    "    # Threshold color channel\n",
    "    s_binary = np.zeros_like(s_channel)\n",
    "    s_binary[(s_channel >= s_thresh[0]) & (s_channel <= s_thresh[1])] = 1\n",
    "    # Stack each channel\n",
    "    color_binary = np.dstack(( np.zeros_like(sxbinary), sxbinary, s_binary)) * 255\n",
    "    \n",
    "    \n",
    "    # Combine the two binary thresholds\n",
    "    binary_image = np.zeros_like(sxbinary)\n",
    "    binary_image[(s_binary == 1) | (sxbinary == 1)] = 1\n",
    "    return color_binary, binary_image\n",
    "    \n",
    "def visualizeColorTheshold(image1, image2):\n",
    "    # Plot the result\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    f.tight_layout()\n",
    "\n",
    "    ax1.imshow(image1)\n",
    "    ax1.set_title('Color_binary', fontsize=15)\n",
    "\n",
    "    ax2.imshow(image2, cmap='gray')\n",
    "    ax2.set_title('Binary_image', fontsize=15)\n",
    "    # plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.copy(undistorted_straight_lines2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_binary, binary_image = pipeline(img)\n",
    "visualizeColorTheshold(color_binary, binary_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Perspective transform\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import perspective_transform_functions as ptf\n",
    "import draw_lane_lines as draw_lane_lines\n",
    "\n",
    "# Helper functions\n",
    "def visualizePerspectiveTransform(image1, image2):\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    f.tight_layout()\n",
    "    ax1.imshow(image1)\n",
    "    ax1.set_title('Original Undistorted Image', fontsize=15)\n",
    "    ax2.imshow(image2)\n",
    "    ax2.set_title('Undistorted and Warped Image', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perspective transform \n",
    "top_down = ptf.get_transformed_perspective(img)\n",
    "visualizePerspectiveTransform(img, top_down)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Region of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get source and destination vertices \n",
    "# src, dst = ptf.get_trapezoid(img)\n",
    "\n",
    "# masked_image = draw_lane_lines.region_of_interest(binary_image, src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[563.2, 475.2],\n",
       "       [716.8, 475.2],\n",
       "       [294.4, 676.8],\n",
       "       [985.6, 676.8]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    src = np.float32([[(img_size[0] / 2) - 55, img_size[1] / 2 + 100],\n",
    "                      [((img_size[0] / 6) - 10), img_size[1]],\n",
    "                      [(img_size[0] * 5 / 6) + 60, img_size[1]],\n",
    "                      [(img_size[0] / 2 + 55), img_size[1] / 2 + 100]])\n",
    "    \n",
    "    dst = np.float32([[(img_size[0] / 4), 0],\n",
    "                      [(img_size[0] / 4), img_size[1]],\n",
    "                      [(img_size[0] * 3 / 4), img_size[1]],[(img_size[0] * 3 / 4), 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a blank mask to start with\n",
    "mask = np.zeros_like(img)   \n",
    "ignore_mask_color = (255,)\n",
    "\n",
    "#filling pixels inside the polygon defined by \"vertices\" with the fill color    \n",
    "cv2.fillPoly(mask, \n",
    "             np.int32([src]), \n",
    "             ignore_mask_color)\n",
    "\n",
    "#returning the image only where mask pixels are nonzero\n",
    "masked_image = cv2.bitwise_and(img, mask)\n",
    "plt.imshow(masked_image);\n",
    "# plot_imgs(test_img, \"Original Image\", masked_image, \"Region of interest\", image_2_cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_lines(img, lines, color=[255, 0, 0], thickness=3):\n",
    "    \"\"\"\n",
    "    NOTE: this is the function you might want to use as a starting point once you want to \n",
    "    average/extrapolate the line segments you detect to map out the full\n",
    "    extent of the lane (going from the result shown in raw-lines-example.mp4\n",
    "    to that shown in P1_example.mp4).  \n",
    "    \n",
    "    Think about things like separating line segments by their \n",
    "    slope ((y2-y1)/(x2-x1)) to decide which segments are part of the left\n",
    "    line vs. the right line.  Then, you can average the position of each of \n",
    "    the lines and extrapolate to the top and bottom of the lane.\n",
    "    \n",
    "    This function draws `lines` with `color` and `thickness`.    \n",
    "    Lines are drawn on the image inplace (mutates the image).\n",
    "    If you want to make the lines semi-transparent, think about combining\n",
    "    this function with the weighted_img() function below\n",
    "    \"\"\"\n",
    "    \n",
    "    # Copy image \n",
    "    #img = np.copy(img)\n",
    "    \n",
    "    # blank image with original size\n",
    "    #line_image = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "    for line in lines:\n",
    "        for x1,y1,x2,y2 in line:\n",
    "            cv2.line(img, (x1, y1), (x2, y2), color, thickness)\n",
    "    \n",
    "\n",
    "\n",
    "def hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap):\n",
    "    \"\"\"\n",
    "    `img` should be the output of a Canny transform.\n",
    "        \n",
    "    Returns an image with hough lines drawn.\n",
    "    \"\"\"\n",
    "    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)\n",
    "    line_img = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n",
    "    draw_lines(line_img, lines)\n",
    "    #return line_img\n",
    "    return line_img, lines\n",
    "\n",
    "\n",
    "def weighted_img(img, initial_img, α=0.8, β=1., γ=0.):\n",
    "    \"\"\"\n",
    "    `img` is the output of the hough_lines(), An image with lines drawn on it.\n",
    "    Should be a blank image (all black) with lines drawn on it.\n",
    "    \n",
    "    `initial_img` should be the image before any processing.\n",
    "    \n",
    "    The result image is computed as follows:\n",
    "    \n",
    "    initial_img * α + img * β + γ\n",
    "    NOTE: initial_img and img must be the same shape!\n",
    "    \"\"\"\n",
    "    return cv2.addWeighted(initial_img, α, img, β, γ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(3.4.2) /opt/concourse/worker/volumes/live/9523d527-1b9e-48e0-7ed0-a36adde286f0/volume/opencv-suite_1535558719691/work/modules/imgproc/src/hough.cpp:472: error: (-215:Assertion failed) image.type() == (((0) & ((1 << 3) - 1)) + (((1)-1) << 3)) in function 'HoughLinesProbabilistic'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-0a7301df2b69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Output \"lines\" is an array containing endpoints of detected line segments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#draw_lane_lines.hough_lines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mline_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhough_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_line_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_line_gap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# plt.figure(figsize=(12,8))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-a7862b08f486>\u001b[0m in \u001b[0;36mhough_lines\u001b[0;34m(img, rho, theta, threshold, min_line_len, max_line_gap)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mReturns\u001b[0m \u001b[0man\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mhough\u001b[0m \u001b[0mlines\u001b[0m \u001b[0mdrawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \"\"\"\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHoughLinesP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminLineLength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_line_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxLineGap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_line_gap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mline_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mdraw_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(3.4.2) /opt/concourse/worker/volumes/live/9523d527-1b9e-48e0-7ed0-a36adde286f0/volume/opencv-suite_1535558719691/work/modules/imgproc/src/hough.cpp:472: error: (-215:Assertion failed) image.type() == (((0) & ((1 << 3) - 1)) + (((1)-1) << 3)) in function 'HoughLinesProbabilistic'\n"
     ]
    }
   ],
   "source": [
    "rho = 2 # distance resolution in pixels of the Hough grid\n",
    "theta = np.pi/180 # angular resolution in radians of the Hough grid\n",
    "threshold = 10    # minimum number of votes (intersections in Hough grid cell)\n",
    "min_line_len = 10 #minimum number of pixels making up a line\n",
    "max_line_gap = 20    # maximum gap in pixels between connectable line segments\n",
    "\n",
    "\n",
    "# Run Hough on edge detected image\n",
    "# Output \"lines\" is an array containing endpoints of detected line segments\n",
    "#draw_lane_lines.hough_lines\n",
    "line_img, lines = hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap)\n",
    "\n",
    "# plt.figure(figsize=(12,8))\n",
    "# plt.imshow(line_img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(line_img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.copy(undistorted_straight_lines1)\n",
    "# img = np.copy(binary_image)\n",
    "continous_lines = getLeft_and_rightLane(img, lines)\n",
    "\n",
    "final_image = drawSingleft_and_rightLane(img,continous_lines,thickness=6,)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(final_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
